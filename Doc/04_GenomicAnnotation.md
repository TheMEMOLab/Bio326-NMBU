# Prokaryotic functional genome annotation

### So far, we have obtained individual metagenome assembled genomes (MAGs) from the rumen metagenome. Let's review our workflow to see what is the next step: 

![workflow](https://github.com/avera1988/NMBU-Bio-326/blob/main/images/wrokflowmetagenome.png) 

After binning with [MetaBat2](https://bitbucket.org/berkeleylab/metabat/src/master/) we obtained a set of bins. We used [CheckM](https://ecogenomics.github.io/CheckM/) to assess the quality of these MAGs.

Let's take a look at the Metabat2 bins:

```bash
ls -1 $SCRATCH/prok/results/metabat2/|grep .fa|sort -V
bin.1.fa
bin.2.fa
bin.3.fa
bin.4.fa
bin.5.fa
bin.6.fa
bin.7.fa
bin.8.fa
bin.9.fa
bin.10.fa
bin.11.fa
bin.12.fa
bin.13.fa
bin.14.fa
bin.15.fa
bin.16.fa
bin.17.fa
bin.18.fa
bin.19.fa
bin.20.fa
bin.21.fa
bin.22.fa
bin.23.fa
bin.24.fa
bin.25.fa
bin.26.fa
bin.27.fa
bin.28.fa
bin.29.fa
bin.30.fa
bin.31.fa
bin.32.fa
bin.33.fa
```

We then can use the report from the ```assemblycomparator2``` to check the quality report by CheckM. We can use either the report table or directly the CheckM results. 

**If you have faced difficulties to run the assemblycomparator2 pipeline, do not worry, you can get a copy of the results like this: ```cp -r $COURSES/BIO326/PROK/data/metagenomic_assembly/demo/results_ac2 $SCRATCH/prok/results``` :-)** 

Let's go to the ```cd results_ac2/``` folder and enter the checkM results:

```bash
cd $SCRATCH/prok/results/metabat2/results_ac2
ls checkm2/
```
```console
checkm2.log  diamond_output  protein_files  quality_report.tsv
```
Take a look into the report
```bash
head checkm2/quality_report.tsv
```

```console
Name    Completeness    Contamination   Completeness_Model_Used Translation_Table_Used  Coding_Density  Contig_N50      Average_Gene_Length      Genome_Size     GC_Content      Total_Coding_Sequences  Additional_Notes
bin.1   63.25   6.18    Neural Network (Specific Model) 11      0.789   249821  249.17762660619803      1250648 0.32    1323    None
bin.10  77.6    23.62   Neural Network (Specific Model) 11      0.896   153900  266.68679245283016      1887020 0.38    2120    None
bin.11  62.73   91.55   Neural Network (Specific Model) 11      0.866   124377  269.06716472570133      12690339        0.34    13653    Low confidence prediction - substantial (37%) disagreement between completeness prediction models
bin.12  100.0   73.05   Gradient Boost (General Model)  11      0.843   26750   217.2540721457791       10324557        0.5     13445    Low confidence prediction - substantial (34%) disagreement between completeness prediction models
bin.13  73.53   49.94   Gradient Boost (General Model)  11      0.815   34058   215.08106551475882      5458247 0.53    6945    None
bin.14  82.65   56.19   Neural Network (Specific Model) 11      0.794   30905   204.71395348837208      6279095 0.3     8170    None
bin.15  91.71   20.06   Gradient Boost (General Model)  11      0.868   120751  297.4990011985617       2566030 0.53    2503    None
bin.16  39.79   9.3     Neural Network (Specific Model) 11      0.733   27245   195.06296851574214      1059231 0.3     1334    None
bin.17  56.04   5.82    Gradient Boost (General Model)  11      0.822   44596   251.9482619240097       1132746 0.55    1237    None
```

Now we can start filtering the MAGs. A good criterion is to use the quality and contamination of the MAGs to sort them into *High*, *Medium* and *Low* quality MAGs. 
We can use the following table from [Bowers et al.,](https://www.nature.com/articles/nbt.3893) to classify the MAGs:
![tablemags](https://github.com/avera1988/NMBU-Bio-326/blob/main/images/mags.jpg)

We have all this information from the table generated by CheckM, and we can use it to extract these quality score parameters. We can easily pick manually those "good quality" MAGs and sort them for quality and contamination scores. Let's say >= 80 % completeness and < 10 % contamination by looking into this table and applying some conditional using awk:

```bash
cat checkm2/quality_report.tsv | awk '{if($2 >= 80 && $3 < 10) print $1"\t"$2"\t"$3}'
```

```console
bin.21  84.46   5.2
bin.6   87.84   2.07
```
**We can see here that only 2 bins meet the condition of > 80 % completeness and < 10 % contamination.**

We can then predict genes and annotate this by comparing with public databases. For doing this we will use DRAM...

## DRAM: Distilled and Refined Annotation of Metabolism

**"[DRAM](https://github.com/WrightonLabCSU/DRAM) (Distilled and Refined Annotation of Metabolism) is a tool for annotating metagenomic assembled genomes and VirSorter identified viral contigs. DRAM annotates MAGs and viral contigs using KEGG (if provided by the user), UniRef90, PFAM, dbCAN, RefSeq viral, VOGDB and the MEROPS peptidase database as well as custom user databases..."**
 ![dramaanot](https://github.com/avera1988/NMBU-Bio-326/blob/main/images/DRAM.jpg)

To use DRAM we first need to create a directory and place there those bins we selected by completeness and contamination scoring (in this example rumen.3 and rumen.6). These bins are in the metabat2 results folder, so let's create a new folder in the ```$SCRATCH/prok/results``` named ```bins_for_dram``` and move there those selected bins:

```bash

mkdir $SCRATCH/prok/results/bins_for_dram
#Moving reads
cat $SCRATCH/prok/results/metabat2/results_ac2/checkm2/quality_report.tsv | awk '{if($2 >= 80 && $3 < 10) print $1}'|while read -r line ;do cp $SCRATCH/prok/results/metabat2/$line.fa $SCRATCH/prok/results/bins_for_dram/ ;done

```

Let's check these files were copied:

```bash
ls $SCRATCH/prok/results/bins_for_dram/
```
```console
bin.21.fa  bin.6.fa
```
With this, we can run DRAM using the following ```sbatch``` script:

```bash
#!/bin/bash

###############SLURM SCRIPT###################################

## Job name:
#SBATCH --job-name=DRAM
#
## Wall time limit:
#SBATCH --time=48:00:00
#
## Other parameters:
#SBATCH --cpus-per-task 16
#SBATCH --mem=80G
#SBATCH -p hugemem-avx2
#SBATCH -o slurm-%x-%A.out

###########################################################

print_usage() {
        echo "Usage: sbatch $0 path_magsdir mags_ext path_out_dir translate_table"
    }

if [ $# -lt 1 ]
        then
                print_usage
                exit 1
elif [ $# -lt 4 ]
    then
        print_usage
                exit 1
elif [ $# -gt 4 ]
    then
        print_usage
                exit 1
    fi


## Set up job environment:

module --quiet purge  # Reset the modules to the system default
module load Miniconda3 && eval "$(conda shell.bash hook)"

##Activate conda environments
conda activate /net/fs-2/scale/OrionStore/Orion/conda/CIGENE/DRAM_1.5

##Declaring variables: These needs to be passed as arguments in the command line

magsdir=$1 #Directory with the MAG e.g /mnt/SCRATCH/auve/DRAM_Test/BinsForDram
ext=$2 #Extension of the MAGs e.g. .fa
outdir=$3 #output directory e.f /mnt/SCRATCH/auve/DRAM_Test/
transtable=$4 #The translation table to predict ORFs


####Do some work:########

## For debuggin
echo "Hello" $USER
echo "my submit directory is:"
echo $SLURM_SUBMIT_DIR
echo "this is the job:"
echo $SLURM_JOB_ID
echo "I am running on:"
echo $SLURM_NODELIST
echo "I am running with:"
echo $SLURM_CPUS_ON_NODE "cpus"
echo "Today is:"
date


## Copying data to local node for faster computation

cd $TMPDIR

#Check if $USER exists in $TMPDIR

if [[ -d $USER ]]
        then
                echo "$USER exists on $TMPDIR"
        else
                mkdir $USER
fi

cd $USER
mkdir tmpDir_of.$SLURM_JOB_ID
cd tmpDir_of.$SLURM_JOB_ID
wd=$(pwd)

#Copy the MAGs to the $TMPDIR

echo "copying MAGs to" $TMPDIR/$USER/tmpDir_of.$SLURM_JOB_ID
mkdir MAGS && cd MAGS
cp -r $magsdir/*$ext .
cd $wd

##################DRAM##############################

echo "DRAM started at"
date +%d\ %b\ %T

time DRAM.py annotate \
-i 'MAGS/*.'$ext \
-o dram.annotation \
--trans_table $transtable \
--min_contig_size 500 \
--threads $SLURM_CPUS_ON_NODE


echo "Distilling..."

time DRAM.py distill \
-i dram.annotation/annotations.tsv \
-o dram.genome_summaries \
--trna_path dram.annotation/trnas.tsv \
--rrna_path dram.annotation/rrnas.tsv

echo "DRAM finished at"
date +%d\ %b\ %T

##Copy data to output dir

mkdir DRAM.Results.dir
mv dram.annotation DRAM.Results.dir
mv dram.genome_summaries DRAM.Results.dir

cp -r DRAM.Results.dir $outdir

echo "DRAM results are in: " $outdir/DRAM.Resulits.dir

##Clean TMPDIR

cd $TMPDIR/$USER
rm -r $wd

##

echo "I've done...Bye!"
date

```

**A copy of this script is at:**

```/mnt/courses/BIO326/PROK/scripts/dram.SLURM.sh```

For running this script, we need to provide in the same command line 3 arguments:
- absolute path of our input directory ```$SCRATCH/prok/results/bins_for_dram```
- The extension of our fasta files ```fa```
- The output directory ```$SCRATCH/prok/results/```
- The translation table for ORFs prediction (default 11)

Let's run the script ```dram.SLURM.sh```

```bash
sbatch dram.SLURM.sh $SCRATCH/prok/results/bins_for_dram fa $SCRATCH/prok/results 11
```

**!NB DRAM will take around 2-8 hrs for running and requires a lot of memory**

When DRAM finishes it will produce the following directory:

```DRAM.Results.dir```

We can then take a look:

```bash
cd $SCRATCH/prok/results/DRAM.Results.dir
ls
```
```console
dram.annotation  dram.genome_summaries
```
There are two directories: 
* dram.annotation.dir: It has all the "raw" annotations, gene sequences, protein preditions of the MAGs
* dram.genome_summaries.dir: It has the distilled part of the genomes with the sorted metabolic functions.

Let's check the annotation directory:

```bash
ls
```
```console
annotations.tsv  genbank  genes.faa  genes.fna  genes.gff  rrnas.tsv  scaffolds.fna  trnas.tsv
```

Here we can find a table with annotations (annotations.tsv) as well as 3 fasta files:

- genes.fna (All the predicted coding genes as nucleotides)
- genes.faa (All the predicted coding genes translated to proteins)
- scaffolds.fna (The total scaffolds/contigs of the bins)

Then take a look into the summaries directory:

```bash
cd $SCRATCH/prok/results/DRAM.Results.dir/dram.genome_summaries
ls
```

```console
genome_stats.tsv  metabolism_summary.xlsx  product.html  product.tsv
```
The files have different information:

* genome_stats.tsv: Basic annotation stats of the genomes, such as # of contigs/scaffolds, taxonomy, RNA genes etc.
* metabolism_summary.xlsx: An excel file with all the Metabolic summary in each genome.
* product.html: Interactive heatmaps of the metabolic summaries
* product.tsv: Tables to reproduce the heatmaps of above

Although we can display the content of the *.tsv* files obtained by DRAM here in the terminal, the metabolism_summary.xlsx and product.html files are visually friendly, so it is recommendable to export these to our personal computers and take a look. 

Once in your computer, you can open the product.html, to explore the metabolic potential of your MAGs.

![dramhtml](https://github.com/TheMEMOLab/Bio326-NMBU/blob/main/images/visualizationDRAM.png)

Is there any special metabolic pathway you would you like to look at? 

Now is time for the fun part that is parsing the information and interpreting the biological meaning encoded in these MAGs ...

## Using KEGG ID 

We can use the KEGG id to map and infer metabolic functions using the [KEGG](https://www.kegg.jp/kegg/mapper/reconstruct.html) mapper portal. 

Let's extract the KO numbers for the bin21...

```bash
cd $SCRATCH/prok/results/DRAM.Results.dir/dram.annotation
cat annotations.tsv |cut -f 1,9|grep bin.21|awk '{if($2 ~ /^K/) print $1"\t"$2}' > bin21.
ko.txt
head bin21.ko.txt
```
```console
bin.21_contig_10234_13  K15635
bin.21_contig_10234_14  K03431
bin.21_contig_10234_16  K23144
bin.21_contig_10234_17  K01726
bin.21_contig_10234_18  K00817
bin.21_contig_10234_19  K04069
bin.21_contig_10234_23  K09125
bin.21_contig_10234_24  K10747
bin.21_contig_10234_25  K03147
bin.21_contig_10234_27  K02342
```

Transfer this file to your computer. A guide on how to transfer files can be found here: [Orion-Transfer_data](https://orion.nmbu.no/en/Copydata)

Now open the [KEGG](https://www.kegg.jp/kegg/mapper/reconstruct.html) mapper porta and load your file and execute.

![KEGGMAPER1](https://github.com/TheMEMOLab/Bio326-NMBU/blob/main/images/KEGGMapper1.PNG)

KEGG will sumarize all the annotated genes. 

Let's take a look in for example the Methane metabolism:

![METHANE](https://github.com/TheMEMOLab/Bio326-NMBU/blob/main/images/KEGGMapper2.PNG)

What other metabolic pathways you can detect?
